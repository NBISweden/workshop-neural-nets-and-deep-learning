{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Good practices of NN/DL project design\n",
    "## What to do and - more importantly perhaps - not to do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=figures/oldman.webp></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Is my project right for Neural Networks?\n",
    "\n",
    "* The thought process should not be: “I have some data, why don’t we try neural networks”\n",
    "* But it should be: “Given the problem, does it make sense to use neural networks?”\n",
    "\n",
    "    * Do I really need non-linear modelling?\n",
    "    * What literature is out there for similar problems?\n",
    "    * How much data will I be able to gather or put my hands on?\n",
    "    * Are there datasets out there that I can re-use before I collect my data?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Do I really need non-linear modelling?\n",
    "\n",
    "* Sometimes linear methods perform just as well if not better\n",
    "* Less risk of catastrophic overfitting\n",
    "* Faster to code, optimize, run, debug\n",
    "* Use linear modelling as a baseline before you move to non-linear methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Real-life example\n",
    "\n",
    "Drop-in question: \"I tried deep learning on my data and it didn't perform better than this other simpler method\"\n",
    "\n",
    "* Classifying gene expression samples\n",
    "* Thousands features\n",
    "* 1000 samples\n",
    "* 2 classes\n",
    "* NN looked like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cell_style": "split",
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 1000)              5001000   \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 1002      \n",
      "=================================================================\n",
      "Total params: 5,502,502\n",
      "Trainable params: 5,502,502\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(1000, input_dim=5000))\n",
    "model.add(Dense(500))\n",
    "model.add(Dense(2, activation=\"softmax\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Parameters (weights) vs. samples\n",
    "\n",
    "* If the number of parameters is many times higher than the number of samples a NN will never work\n",
    "* Ideally, we are looking for the inverse: way more samples than parameters\n",
    "* Some rules of thumb out there:\n",
    "    * Definitely bad if number of weights > number of samples\n",
    "    * 10x as many labelled samples as there are weights\n",
    "    * A few thousand samples per class\n",
    "    * Just try it and downscale/regularize until you're not overfitting anymore (or until you have a linear model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## And even if I have enough data for a NN...\n",
    "\n",
    "... is Deep Learning the right choice?\n",
    "\n",
    "* The tasks were Deep Learning shine are those that require feature extraction:\n",
    "    * Imaging -> edge/object detection\n",
    "    * Audio/text -> sound/word/sentence detection\n",
    "    * Protein structure prediction -> mutation patterns/local structure/global structure\n",
    "\n",
    "* Deep Learning makes feature extraction automatic and seem to work best when there is a hierarchy to these features\n",
    "* Is your data made that way? \n",
    "    * Does it have an order (spatial/temporal)? \n",
    "    * Are smaller patterns going to form higher-order patterns?\n",
    "* All these different types of layers need to be there for a reason\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"figures/feature_extraction.png\"></img>\n",
    "\n",
    "source: [datarobot](https://www.datarobot.com/blog/a-primer-on-deep-learning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## And even when both these conditions have been met\n",
    "\n",
    "... you need a few more things:\n",
    "\n",
    "* Domain knowledge is not enough\n",
    "* Sometimes people with NN/DL knowledge and no domain knowledge end up being the right ones for the job (see Alphafold)\n",
    "* You also need lots of patience and time, these things rarely work out of the box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A few more things to keep in mind\n",
    "\n",
    "* You need extensive knowledge of your data:\n",
    "    * Split the data in a rigorous way to avoid introducing biases\n",
    "    * Check for _information leakage_ before you get overly optimistic results\n",
    "    * Make sure that there are no errors in your data\n",
    "\n",
    "And therein lies the main issue:\n",
    "* Some think that DL is about having a model magically fixing your data\n",
    "* Instead, DL is _mostly_ about knowing your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1) Neural Nets are very good at detecting patterns and they will use this against you\n",
    "\n",
    "### (a.k.a. target leakage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Target leakage\n",
    "\n",
    "* Making a predictor when you know the answers is not as easy as it seems\n",
    "* Need to remove any revealing info you would not have access to in real scenario\n",
    "* Classic example: predict yearly salary of employee\n",
    "    * But one of the features is \"monthly income\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: detecting COVID-19 from chest scans \n",
    "(https://www.datarobot.com/blog/identifying-leakage-in-computer-vision-on-medical-images/)\n",
    "\n",
    "* COVIDx dataset\n",
    "* Training set: chest X-rays of 66 positive COVID results, 120 random non-COVID examples\n",
    "* 2-class classifier based on ResNet50 Featurizer\n",
    "* Perfect validation results! Great!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: detecting COVID-19 from chest scans \n",
    "\n",
    "Inspecting dataset with image embeddings tells another story: can anyone tell what's wrong?\n",
    "\n",
    "<img src=\"figures/covidchest.png\"></img>\n",
    "[(source)](https://www.datarobot.com/blog/identifying-leakage-in-computer-vision-on-medical-images/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: detecting COVID-19 from chest scans \n",
    "\n",
    "Let's look at activations map and see more in detail\n",
    "* Get final layer's output after activation (ReLU) and plot figure\n",
    "\n",
    "<img src=\"figures/covidchest2.png\"></img>\n",
    "[(source)](https://www.datarobot.com/blog/identifying-leakage-in-computer-vision-on-medical-images/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: normalizing inputs on train/validation/test data\n",
    "\n",
    "* If you normalize on validation data as well you are getting information you wouldn't have in a real scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lab 1: looking for target leakage in a text dataset (~1 h.)\n",
    "\n",
    "Jupyter notebook (download from canvas module page)\n",
    "\n",
    "Visualize the layers of a NN for Natural Language Processing:\n",
    "\n",
    "* Can you tell if there is target leakage of some kind?\n",
    "* Propose solutions to curb the issue\n",
    "\n",
    "Proposed solutions:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2) Know your train/validation/test sets\n",
    "\n",
    "* A _train set_ is a set of samples used to tune the NN weights\n",
    "* A _validation set_ is a set used to tune the NN hyperparameters:\n",
    "    * Type of model (maybe not even a NN)\n",
    "    * Number of layers\n",
    "    * Number of neurons per layer\n",
    "    * Type of layers\n",
    "    * Optimizer\n",
    "    * Validation set results are NOT the ones that will get published\n",
    "    * Doesn't matter if you cross-validate\n",
    "* A _test set_ is a secluded set of samples that are used only once to test the final model\n",
    "    * Give an idea of how well the model generalizes to unseen data (results go on paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Beware of similar samples across sets\n",
    "\n",
    "<img src=\"figures/homer.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>\n",
    "<br>\n",
    "<img src=\"figures/guyincognito.png\">\n",
    "(2F08 “Fear of Flying”)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Knowing what each set does is half the battle\n",
    "\n",
    "Train, validation and test sets cannot be too similar to each other, or you will not be able to tell if the network is generalizing or just memorizing\n",
    "\n",
    "* _How_ different they should be depends on what you're trying to achieve\n",
    "* Come up with a similarity measure\n",
    "* At the very least remove duplicate samples\n",
    "* You would be surprised how often scientists mess this up\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"figures/andrewng.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=\"figures/trainvalidationleak1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"figures/andrewng.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=\"figures/trainvalidationleak.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sad ending :(\n",
    "<img src=\"figures/trainvalidationleak2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Another example, protein structure prediction\n",
    "\n",
    "* For some reason most researchers try to split train/validation/test by sequence similarity\n",
    "* If two proteins have <25% identical amino acids, they are deemed different enough\n",
    "* But protein families/superfamilies contain many proteins that share no detectable sequence similarity\n",
    "* Sequence similarity is not the right metric!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"figures/25percent.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lab 2: splitting a protein sequence dataset (~1 h.)\n",
    "\n",
    "Jupyter notebook:\n",
    "\n",
    "session_goodPracticesDatasetDesign/lab_validation/rigorous_train_validation_splitting.ipynb\n",
    "\n",
    "Two different strategies will be tested:\n",
    "* Random split\n",
    "* Split by alignment score\n",
    "\n",
    "Which works best? Different groups test different networks on each strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3) Your model is only as good as your data \n",
    "\n",
    "Reasons why one of my networks wouldn't work:\n",
    "\n",
    "* Labels were wrong (label for amino acid n was assigned to amino acid n+1)\n",
    "* The actual target sequence was missing from the multiple sequence alignment\n",
    "* Inputs weren't correctly scaled/normalized\n",
    "* Script to convert 3-letter code amino acid to one letter (LYS -> K) didn't work as expected\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=\"figures/unknown.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## NNs are robust\n",
    "\n",
    "They will \"kind of\" work even when some labels are incorrect, but it is going to be very tricky to understand if and what is wrong\n",
    "\n",
    "* Before training:\n",
    "    * Plot data distributions\n",
    "    * Test all data preparation scripts\n",
    "    * Manually look at data files\n",
    "    * Check labels for mistakes, unbalancedness\n",
    "\n",
    "* While training:    \n",
    "    * Look at badly predicted samples\n",
    "    * Be paranoid when something doesn't work well, even more when it works surpisingly well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "<br>\n",
    "<br>\n",
    "<img src=\"figures/monk.jpg\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ok, my data is perfect but I don't have enough of it: what now?\n",
    "\n",
    "Main avenues:\n",
    "* Find more of it\n",
    "* Make smaller models\n",
    "* Cut down insignificant features\n",
    "* Generate artificial samples: Data augmentation\n",
    "* Transfer learning (so find more data, again)\n",
    "* Think outside the (black) box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Feature selection\n",
    "\n",
    "* We are moving away from Deep Learning (automatic feature extraction from raw data)\n",
    "* Remove highly correlated inputs first, that's easy\n",
    "* Keep in mind that categorical inputs are more \"costly\" in terms of parameters\n",
    "    * E.g. a 10-category input will be encoded as 10 separate inputs (one-hot)\n",
    "* Feature ablation studies\n",
    "* Autoencoders to compress inputs?\n",
    "* Feature importance through other ML methods:\n",
    "    * Random Forest\n",
    "    * Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Feature ablation\n",
    "\n",
    "* Ablation study (on features):\n",
    "    * Remove parts of the inputs, see what happens\n",
    "    * If results improve, remove some other inputs\n",
    "    * If not, try removing other inputs and so on\n",
    "\n",
    "* Could be implemented in annealing procedure to speed things up\n",
    "* As usual, do this only on training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regularizers (https://keras.io/api/layers/regularizers/)\n",
    "\n",
    "You thought we were done with Keras api explanations, but we ain't\n",
    "\n",
    "* Regularizers are used to constrain the training so that weights don't get too big (a cause of overfitting)\n",
    "* L1 regularization (Lasso): \n",
    "    * $L_r(x,y) = L(x,y) + \\lambda  \\sum_{i,j} \\lvert w_{i,j}\\rvert $\n",
    "    * Results in sparse weight matrices (many weights to 0)\n",
    "* L2 regularization (Ridge):\n",
    "    * $L_r(x,y) = L(x,y) + \\lambda  \\sum_{i,j} w_{i,j}^2 $\n",
    "    * Results in smaller weights\n",
    "    \n",
    "* Let's have a look on [tensorflow playground](http://playground.tensorflow.org)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "from keras import regularizers\n",
    "\n",
    "layer = Dense(\n",
    "    units=64,\n",
    "    kernel_regularizer=regularizers.l1(1e-5), #the parameter is the lambda\n",
    "    bias_regularizer=regularizers.l2(1e-4),\n",
    "    activity_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4) #two lambdas here\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data augmentation\n",
    "\n",
    "* We have already seen it in action during the Convolutional lab\n",
    "    * If we have few images, we can flip them, rotate them, shift them...\n",
    "    * Extra instances that the network will benefit from\n",
    "* Pixel-wise classification was also a form of augmentation!\n",
    "    * 100 images of size (640x480) suddenly become 100x640x480 labelled samples\n",
    "* Extreme examples: generative neural networks (see VAE from yesterday)\n",
    "* Similar things might be done to other kinds of data as well\n",
    "    * Can you imagine ways to augment the data you work with?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Transfer learning\n",
    "\n",
    "* Say you have a small labelled dataset for a specific problem\n",
    "* But there are larger datasets out there for similar applications\n",
    "* Transfer learning means training a large Neural Network on the large set, then use parts of it on the small set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Transfer learning\n",
    "\n",
    "* Train a deep classifier on a large dataset\n",
    "* The bottom (first) layers of the network learn to extract relevant features\n",
    "* The top (last) layers learn to classify\n",
    "* Keep the bottom layers, freeze them (so that the weight can't change anymore)\n",
    "* Re-initialize the top layers weights randomly\n",
    "* Retrain the network on the small dataset so that only the top layers weights are now trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"figures/transferlearning.png\"></img>\n",
    "[(img source)](https://www.slideshare.net/xavigiro/transfer-learning-d2l4-insightdcu-machine-learning-workshop-2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Think outside the box\n",
    "\n",
    "* Neural Networks allow you to be very flexible and creative\n",
    "* Say we have images of cells as in the convolutional lab, but too few of them ($n$ samples)\n",
    "    * Build a network to classify whether two cells are of the same kind\n",
    "    * Suddenly we have a dataset with $n^2$ samples instead of $n$\n",
    "    * Two branches of the network (one per input image) with a merged output layer\n",
    "    * The two branches can actually be the _same_ network repeated twice\n",
    "    * When you're done with training it, do transfer learning for the actual classification task\n",
    "    * Maybe issues with information leakage? Would be good to check\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Lab 3: transfer learning in imaging data (~1 h.)\n",
    "\n",
    "Jupyter notebook: \n",
    "\n",
    "session_goodPracticesDatasetDesign/lab_transferLearning/transfer_learning.ipynb\n",
    "\n",
    "* Christophe's lab on cell classification:\n",
    "    * We want to train a larger network\n",
    "    * Use a network pre-trained on completely different data\n",
    "    * Is it going to help?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tips and tricks on training your Neural Networks\n",
    "\n",
    "* Know your data\n",
    "* Fix random seeds for reproducibility\n",
    "* Manually calculate metric for baseline naïve predictor\n",
    "* Overfit first, ask questions later:\n",
    "    * Training on small dataset (one batch of data) first: can you make it overfit?\n",
    "    * Can you make it overfit on the full dataset?\n",
    "    * Now scale it back (fewer layers/neurons etc)\n",
    "* Look obsessively at training curves, compare multiple tests\n",
    "* Look at samples where prediction fails, why are they special?\n",
    "* Change one thing at a time!\n",
    "* Be patient and let the model train when you have a reasonable one\n",
    "* Neural Networks are not necessarily black boxes, visualize outputs from different layers to see where the network is focusing\n",
    "* Ensemble multiple NNs to get better predictions\n",
    "* Think of your labels, should you classify or regress?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## When to use GPUs, when CPUs?\n",
    "\n",
    "* Use GPUs if it saves you a considerable amount of time\n",
    "    * Especially when trying to optimize hyperparameters (fast turn-around)\n",
    "    * Might be ok to use CPU if you want to train a known architecture and have some time\n",
    "    * Access to GPU nodes on clusters might be slow\n",
    "    * If only for inference (not training), CPU should be quick enough\n",
    "* Use CPUs if you run out of memory (OOM errors):\n",
    "    * Bigger GPUs have ~16GB RAM to date\n",
    "    * A small cluster node has usually 128GB RAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training Neural Networks in Sweden\n",
    "\n",
    "Many resources available:\n",
    "\n",
    "* [BerzeLiUs](https://www.nsc.liu.se/systems/berzelius/) (A100, 60 nodes with 8 GPUs each)\n",
    "* [Snowy@UPPMAX](https://www.uppmax.uu.se/support/user-guides/using-the-gpu-nodes-on-snowy/) (Tesla T4s)\n",
    "* [Kebnekaise@HPC2N](https://www.hpc2n.umu.se/resources/hardware/kebnekaise) (V100s/K80s, 46 nodes)\n",
    "* [Tetralith@NSC](https://www.nsc.liu.se/support/systems/tetralith-GPU-user-guide/) (Tesla T4s, 170 nodes)\n",
    "* [Alvis@C3](https://www.c3se.chalmers.se/about/Alvis/) (Tesla T4s, 17+ nodes)\n",
    "* [Google Colab](colab.research.google.com/)\n",
    "* ...?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "<img src=\"figures/snic.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Come to th"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:nn_dl_python] *",
   "language": "python",
   "name": "conda-env-nn_dl_python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
