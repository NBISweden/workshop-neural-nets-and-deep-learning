{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Intro to Keras, Tensorflow and advanced NN\n",
    "\n",
    "\n",
    "<center><img src=\"figures/keras-tensorflow-logo.jpg\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "* TensorFlow is an end-to-end open source platform for ML \n",
    "* It has a comprehensive, flexible ecosystem of tools and libraries \n",
    "* Allows to easily build and deploy ML powered applications.\n",
    "* Not only Neural Networks\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## https://keras.io\n",
    "\n",
    "* Keras is a high-level neural networks API (front-end), written in Python\n",
    "* Capable of running on top of TensorFlow, CNTK, or Theano (backends)\n",
    "* Built to simplify access to more complex backend libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## https://keras.io\n",
    "\n",
    "Use Keras if you need a deep learning library that:\n",
    "\n",
    "* Allows for easy and fast prototyping (through user friendliness, modularity, and extensibility).\n",
    "* Supports both convolutional networks and recurrent networks, as well as combinations of the two.\n",
    "* Runs seamlessly on CPU and GPU.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## https://tensorflow.org\n",
    "\n",
    "Use *TensorFlow* if you want a finer level of control:\n",
    "\n",
    "* Build your own NN layers\n",
    "* Personalized cost function\n",
    "* More complex architectures than those available on Keras\n",
    "    \n",
    "We will be mostly writing python code using Keras libraries, but \"under the hood\" Keras is using tensorflow libraries.\n",
    "\n",
    "The documentation is at [keras.io](https://keras.io)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## If you want to use R instead of python\n",
    "\n",
    "* Keras can run on R as well, it will look quite similar to the code in this notebook\n",
    "* (but, you know, with <- instead of =)\n",
    "* You find the docs here: https://keras.rstudio.com/\n",
    "* We will also have a lab session completely in R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is a tensor\n",
    "\n",
    "The main variables in TensorFlow are, of course, tensors:\n",
    "\n",
    "> A tensor is often thought of as a generalized matrix. That is, it could be a 1-D matrix (a vector is actually > such a tensor), a 3-D matrix (something like a cube of numbers), even a 0-D matrix (a single number), or a higher dimensional structure that is harder to visualize. The dimension of the tensor is called its rank.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is a tensor\n",
    "\n",
    "The main variables in TensorFlow are, of course, tensors:\n",
    "\n",
    "> A tensor is often thought of as a generalized matrix. That is, it could be a 1-D matrix (a vector is actually > such a tensor), a 3-D matrix (something like a cube of numbers), even a 0-D matrix (a single number), or a higher dimensional structure that is harder to visualize. The dimension of the tensor is called its rank.\n",
    "\n",
    "## TensorFlow operates on tensors\n",
    "\n",
    "> TensorFlow computations are expressed as stateful dataflow graphs. The name TensorFlow derives from the operations that such neural networks perform on multidimensional data arrays, which are referred to as tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The first step is to build a graph of operations\n",
    "\n",
    "* NNs are defined in TensorFlow as graphs through which the data flows until the final result is produced\n",
    "* Before we can do any operation on our data (images, etc) we need to build the graph of tensor operations\n",
    "* When we have a full graph built from input to output, we can run (flow) our data (training or testing) through it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tensors and data are *not* the same thing\n",
    "* Tensors are, rather, a symbolic representation of the data\n",
    "* Think about the function g = f(x): as long as we do not provide a value to x, we will not have a fully computed g\n",
    "* In this case, \"g\" is the output tensor, \"x\" the input tensor, \"f\" the tensor operation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example\n",
    "\n",
    "* We have a set of color images of size 1000x1000 pixels (1 megapixel) that we want to use on our NN \n",
    "* We define tensors with shape (n, 1000, 1000, 3)\n",
    "    * n is the number of images that we are presenting to our network in one go (\"batch block\")\n",
    "    * 1000\\*1000: image pixels\n",
    "    * 3 is the number of channels (RGB)\n",
    "    * Grayscale images tensors would have shape (n, 1000, 1000, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## One thing to remember when operating on tensors\n",
    "\n",
    "The dimensions between tensors coming out of the n-th node and those going into the (n+1)-th node *must* match:\n",
    "\n",
    "* If each sample in our dataset is made of 10 features, the first (input) layer must accept a tensor of shape (n, 10)\n",
    "* If the first layer in our NN outputs a 3D tensor, the second layer must accept a 3D tensor as input\n",
    "* Check the documentation to make sure what input-output shapes are allowed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Here's how a NN layer looks like in TensorFlow:\n",
    "\n",
    "<center><img src=\"figures/run_metadata_graph.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A neural network in Keras is called a Model\n",
    "\n",
    "The simplest kind of model is of the Sequential kind:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This is an \"empty\" model, with no layers, no inputs or outputs are defined either.\n",
    "\n",
    "Adding layer is easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Dense\n",
    "\n",
    "model.add(Dense(units=3, activation='relu', input_dim=3))\n",
    "model.add(Dense(units=2, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "\n",
    "A \"Dense\" layer is a fully connected layer as the ones we have seen in Multi-layer Perceptrons.\n",
    "The above is equal to having this network:\n",
    "\n",
    "<center><img src=\"figures/simplenet.png\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If we want to see the layers in the Model this far, we can just call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Notice the number of parameters, can you tell why 12 and 8 parameters for each layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Using \"model.add()\" keeps stacking layers on top of what we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(units=2, activation=None))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "One can also declare the model in one go, by passing a list of la like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(units=3, activation='relu', input_dim=3),\n",
    "    Dense(units=2, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If we want to see the layers in the Model this far, we can just call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "\n",
    "plot_model(model, \"figures/simplenet_model.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise\n",
    "\n",
    "* Can you build a sequential model to reproduce the graph shown in the figure? \n",
    "* Assume that this is a classifier\n",
    "* Choose whatever activations you want, wherever possible\n",
    "* How many classes are we predicting?\n",
    "\n",
    "<center><img src=\"figures/sequence_api_exercise.png\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import #...\n",
    "\n",
    "#Add your model here\n",
    "\n",
    "plot_model(model, \"figures/exercise_model.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras layers (https://keras.io/api/layers/)\n",
    "\n",
    "Common layers (we will cover all of these!)\n",
    "\n",
    "* Trainable\n",
    "    * <font color='red'>Dense (fully connected/MLP)</font>\n",
    "    * Conv1D (2D/3D)\n",
    "    * MaxPooling1D (2D/3D)\n",
    "    * Recurrent: LSTM/GRU/Bidirectional\n",
    "\n",
    "\n",
    "* Non-trainable\n",
    "    * <font color='red'>Dropout</font>\n",
    "    * Flatten\n",
    "    * <font color='red'>Merge (Add/Multiply/Subtract/Concatenate)</font>\n",
    "    * <font color='red'>Activation (Softmax/ReLU/Sigmoid/...)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Keras layers (https://keras.io/api/layers/)\n",
    "\n",
    "Other useful layers (we will probably not cover these)\n",
    "\n",
    "* Lambda (make your own layer!)\n",
    "* BatchNormalization\n",
    "* Reshape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dropout is a regularization layer\n",
    "\n",
    "* It's applied to a previous layer's output\n",
    "* Takes those outputs and randomly sets them to 0 with probability p\n",
    "* Other outputs are scaled up so that the sum of the inputs remains unchanged\n",
    "* if p = 0.5: model.add(Dropout(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "drop = Dropout(0.5, input_shape=(4,))\n",
    "data = np.arange(1,13).reshape(3, 4).astype(np.float32)\n",
    "\n",
    "print(\"Before:\", data, sep=\"\\n\")\n",
    "output = drop(data, training=True)\n",
    "print(\"After:\", output, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dropout is a regularization layer\n",
    "\n",
    "* Applying the same input twice will give different results\n",
    "* Means that it is harder for the network to memorize patterns\n",
    "* Helps curb overfitting\n",
    "* Especially used with Dense() layers which are prone to overfitting\n",
    "* Works only at training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "import numpy as np\n",
    "tf.random.set_seed(0)\n",
    "drop = Dropout(0.5, input_shape=(4,))\n",
    "data = np.arange(1,13).reshape(3, 4).astype(np.float32)\n",
    "\n",
    "print(\"Before:\", data, sep=\"\\n\")\n",
    "output = drop(data, training=True)\n",
    "print(\"After:\", output, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Keras activations (https://keras.io/api/layers/activations/)\n",
    "\n",
    "Activation functions for regression or inner layers:\n",
    "* Sigmoid\n",
    "* Tanh\n",
    "* ReLU\n",
    "* LeakyReLU\n",
    "* Linear (None)\n",
    "\n",
    "THE activation function for classification (output layer only):\n",
    "* Softmax (ouputs probabilities for each class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Softmax\n",
    "\n",
    "It's an activation function applied to a output vector z with K elements (one per class) and outputs a probability distribution over the classes:\n",
    "\n",
    "<table><tr>\n",
    "<td><img src=\"figures/simplenet.png\" width=200></td>\n",
    "<td><img src=\"figures/softmax.svg\"></td>\n",
    "</tr></table>\n",
    "\n",
    "What makes softmax your favorite activation:\n",
    "\n",
    "* K outputs sums to 1\n",
    "* K probabilities proportional to the exponentials of the input numbers\n",
    "* No negative outputs\n",
    "* Monotonically increasing output with increasing input\n",
    "\n",
    "Softmax is usually only used to activate the last layer of a NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU vs. old-school logistic functions\n",
    "\n",
    "* Historically, sigmoid and tanh were the most used activation functions\n",
    "* Easy derivative\n",
    "* Bound outputs (from -1 to 1)\n",
    "* They look like this:\n",
    "\n",
    "<img src=\"figures/logistic_curve.png\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ReLU vs. old-school logistic functions\n",
    "\n",
    "* Problems arise when we are at large |x|\n",
    "* The derivative in that area becomes small (saturation)\n",
    "* Remember what the chain rule said?\n",
    "\n",
    "<img src=\"figures/logistic_curve.png\" width=400>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ReLU vs. old-school logistic functions\n",
    "\n",
    "* When we have n layers, we have n activation functions\n",
    "* At layer n the derivative is proportional to:\n",
    "$$\\begin{eqnarray} \n",
    "\\frac{\\partial L(w,b|x)}{\\partial w_{ln}} & \\propto &  \\frac{\\partial a_{ln}}{\\partial z_{ln}}\n",
    "\\end{eqnarray}$$\n",
    "* At layer 1 the derivative is proportional to:\n",
    "$$\\begin{eqnarray} \n",
    "\\frac{\\partial L(w,b|x)}{\\partial w_{l1}} & \\propto &  \\frac{\\partial a_{ln}}{\\partial z_{ln}} \\times \\frac{\\partial a_{n-1}}{\\partial z_{ln-1}} \\times \\frac{\\partial a_{ln-2}}{\\partial z_{ln-2}} \\ldots \\times \\frac{\\partial a_{l1}}{\\partial z_{l1}}\n",
    "\\end{eqnarray}$$\n",
    "* It is the product of many numbers < 1\n",
    "* Gradient becomes smaller and smaller for the initial layers\n",
    "* Gradient vanishing problem\n",
    "\n",
    "<!--- <img src=\"figures/large_net.png\" width=400> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ReLU is the first activation to address the issue\n",
    "\n",
    "<center><img src=\"figures/relu.png\" width=400></center>\n",
    "\n",
    "Used in \"internal\" layers, usually not at last layer\n",
    "\n",
    "Pros:\n",
    "* Easy derivative (1 for x > 0, 0 elsewhere)\n",
    "* Derivative doesn't saturate for x > 0: alleviates gradient vanishing\n",
    "* Non-linear\n",
    "\n",
    "Cons:\n",
    "* Non-derivable at 0\n",
    "* Dead neurons if x << 0\n",
    "* Potential gradient explosion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Other ReLU-like activations\n",
    "\n",
    "LeakyReLU/PReLU\n",
    "* Non-zero at x < 0\n",
    "\n",
    "<center><img src=\"figures/leakyrelu.png?0\" width=400></center>\n",
    "\n",
    "ELU\n",
    "* Derivable at 0\n",
    "* Non-zero at x < 0\n",
    "\n",
    "<center><img src=\"figures/elu.png?0\" width=400></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Setting activations in Keras\n",
    "\n",
    "We can add activations as string parameters, or as functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential() \n",
    "model.add(Dense(units=2, activation='sigmoid'))\n",
    "model.add(Dense(units=2, activation='relu'))\n",
    "model.add(Dense(units=2, activation=keras.activations.relu))\n",
    "model.add(Dense(units=2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "But also as separate layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Activation\n",
    "\n",
    "model = Sequential() \n",
    "model.add(Dense(units=2))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(units=2))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(units=2))\n",
    "model.add(Activation(keras.activations.relu))\n",
    "model.add(Dense(units=2))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Once we have defined a model we want to \"compile\" it\n",
    "\n",
    "This means chosing a Loss function and an Optimizer (the algorithm that finds the minimum loss possible)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',                    #adaptive learning rate method\n",
    "              loss='sparse_categorical_crossentropy', #loss function for classification problems with integer labels\n",
    "              metrics=['accuracy'])                   #the metric doesn't influence the training\n",
    "\n",
    "model.optimizer.get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Passing classes as parameters\n",
    "\n",
    "* Some parameters can be set by passing a string (optimizer='rmsprop')\n",
    "* we need to explicitly import the object if we want better control (optimizer=RMSprop())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import RMSprop\n",
    "model.compile(optimizer=RMSprop(),                    #adaptive learning rate method\n",
    "              loss='sparse_categorical_crossentropy', #loss function for classification problems with integer labels\n",
    "              metrics=['accuracy'])                   #the metric doesn't influence the training\n",
    "\n",
    "model.optimizer.get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Passing classes as parameters\n",
    "\n",
    "* Some parameters can be set by passing a string (optimizer='rmsprop')\n",
    "* we need to explicitly import the object if we want better control (optimizer=RMSprop())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import RMSprop\n",
    "model.compile(optimizer=RMSprop(learning_rate=1.0),   #adaptive learning rate method\n",
    "              loss='sparse_categorical_crossentropy', #loss function for classification problems with integer labels\n",
    "              metrics=['accuracy'])                   #the metric doesn't influence the training\n",
    "\n",
    "model.optimizer.get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Losses (https://keras.io/api/losses/)\n",
    "\n",
    "These are the functions used to evaluate and train the neural network\n",
    "\n",
    "Common losses for classification problems:\n",
    "* CategoricalCrossentropy\n",
    "* SparseCategoricalCrossentropy\n",
    "* KLDivergence\n",
    "\n",
    "Common losses for regression problems:\n",
    "* MeanSquaredError\n",
    "* MeanAbsoluteError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Metrics (https://keras.io/api/metrics/)\n",
    "\n",
    "Common metrics for classification:\n",
    "* Accuracy/CategoricalAccuracy (respectively for integer labels or one-hot labels)\n",
    "* SparseCategoricalCrossentropy/CategoricalCrossentropy (integer/one-hot labels)\n",
    "* Precision/Recall\n",
    "* AUC\n",
    "\n",
    "Common metrics for regression:\n",
    "* MeanSquaredError\n",
    "* MeanAbsoluteError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Metrics (https://keras.io/api/metrics/)\n",
    "\n",
    "Notice the \"metrics\" parameter, which accepts a list of values. Multiple metrics can be shown during training.\n",
    "Metrics are only to visualize how the training is going, they don't have an effect on training itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import RMSprop\n",
    "model.compile(optimizer=RMSprop(learning_rate=1.0),   #adaptive learning rate method\n",
    "              loss='sparse_categorical_crossentropy', #loss function for classification problems with integer labels\n",
    "              metrics=['accuracy', 'recall'])         #the metric doesn't influence the training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimizers (https://keras.io/api/optimizers/)\n",
    "\n",
    "* They are algorithms for gradient descent\n",
    "* A few to choose from:\n",
    "    * SGD (Stochastic Gradient Descent)\n",
    "    * RMSprop (Root Mean Square propagation)\n",
    "    * Adadelta (Adaptive delta)\n",
    "    * Adam (Adaptive Moment estimation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=\"figures/gradient_descent.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Batch Gradient Descent \n",
    "\n",
    "We have seen how gradient descent works:\n",
    "\n",
    "For each epoch:\n",
    "* Get predicted y (Å·) for all N samples\n",
    "* Calculate error (loss)\n",
    "* Calculate all gradients (backprop)\n",
    "* Apply gradients to weights\n",
    "    \n",
    "Pros/cons:\n",
    "* Stable procedure\n",
    "* Guarantees lower error at next step\n",
    "* Will get stuck at local minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=\"figures/gradient_descent.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stochastic Gradient Descent\n",
    "For each epoch:\n",
    "* Divide data in batch blocks of size n < N\n",
    "* For each of the N/n blocks:\n",
    "    * Get predicted y for n samples\n",
    "    * Calculate partial loss\n",
    "    * Calculate gradients (backprop)\n",
    "    * Apply gradients to weights\n",
    "\n",
    "Pros/cons:\n",
    "* Noisy gradients\n",
    "* Error will still go down overall\n",
    "* Less likely to get stuck at local minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=\"figures/gradient_descent.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimizers (https://keras.io/api/optimizers/)\n",
    "\n",
    "We need to choose a learning rate to multiply to our gradient. If it is too small, we risk taking too long to get to a minimum\n",
    "<center><img src=\"figures/small_lr.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimizers (https://keras.io/api/optimizers/)\n",
    "\n",
    "If it is too large, the network risks becoming unstable, explode\n",
    "\n",
    "<center><img src=\"figures/large_lr.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimizers \n",
    "\n",
    "Luckily there are algorithms to address these issues:\n",
    "* Increase descent speed when past gradients agree with current, slow down otherwise (momentum)\n",
    "* Annealing (decrease learning rate with passing time)\n",
    "* Different learning rates for different parameters\n",
    "* Adaptive learning rate based on gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## (https://keras.io/api/optimizers/)\n",
    "\n",
    "<img src=\"figures/adaptive_lr.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimizers (https://keras.io/api/optimizers/)\n",
    "\n",
    "* They are algorithms for gradient descent\n",
    "* A few to choose from:\n",
    "    * SGD (stochastic gradient descent)\n",
    "        * One learning rate, fixed\n",
    "        * Old, but works well with Nesterov momentum\n",
    "    * RMSprop\n",
    "        * One learning rate per parameter\n",
    "        * Adaptive learning rate (divide by squared mean of past gradients)\n",
    "    * Adadelta (adaptive learning rate)\n",
    "        * Similar to RMSprop, no need to set initial learning rate\n",
    "    * Adam (Adaptive moment estimation)\n",
    "        * Combines pros from RMSprop, Adadelta, works well with most problems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimizers (https://keras.io/api/optimizers/)\n",
    "\n",
    "<center><img src=\"figures/adam_et_al.png\" width=500></center>\n",
    "<div style=\"text-align: right\">(\"Adam: A Method for Stochastic Optimization\", 2015)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The functional API in Keras\n",
    "\n",
    "* Sequential() is quite simple, but limited\n",
    "* What if we want to have multiple input/output layers?\n",
    "* What if we want a model that is not just a linear sequence of layers?\n",
    "\n",
    "<img src=\"figures/functional_api_40_0.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Functional API (https://keras.io/guides/functional_api)\n",
    "\n",
    "* Layers are now treated as... functions\n",
    "* Now the tensors are explicit variables\n",
    "* Remember: g = f(x)\n",
    "* Careful not to mix up tensors (arguments) and layers (functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x = tf.constant([[2.0, 3.0, 4.0]]) #this is a tensor containing constant values\n",
    "f = Dense(1, input_dim=3) #this is a layer with 3 inputs, 1 output\n",
    "g = f(x)\n",
    "\n",
    "print(x)\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now, let's go back to the first neural network we built with Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units=3, activation='relu', input_dim=3))\n",
    "model.add(Dense(units=2, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "And here is how we make the same model with the functional API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input\n",
    "\n",
    "input_tensor = Input((3,)) #input is a bit like a layer, but it is a function that returns a tensor\n",
    "\n",
    "dense1 = Dense(3, activation='relu') #Dense is a layer constructor, does NOT return a tensor but a layer\n",
    "dense2 = Dense(2, activation='relu')\n",
    "\n",
    "dense1_out = dense1(input_tensor) #when you apply a layer to a tensor, you get another tensor as output\n",
    "dense2_out = dense2(dense1_out)\n",
    "output_tensor = dense2_out\n",
    "\n",
    "model = keras.Model(inputs=input_tensor, outputs=output_tensor, name=\"functional API model\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Note:\n",
    "* Layer functions (Dense(...)) return layers...\n",
    "* ... except for Input(), which returns a tensor instead\n",
    "* We have to explicitly connect the \"inputs\" tensor to the \"outputs\" tensor to form the graph\n",
    "* The input layer is explicitly represented in the first row of the table\n",
    "* The input layer has 0 parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model = keras.Model(inputs=input_tensor, outputs=output_tensor, name=\"functional API model\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "* Can you use the functional API to reproduce the model shown in the figure? \n",
    "* As a hint, I will do all the imports for you, and I will initialize some of the layers\n",
    "* Check the docs (keras.io) to see how each import should be used\n",
    "\n",
    "\n",
    "* It's ok if you can't do it! We are just getting started after all...\n",
    "\n",
    "<img src=\"figures/functional_api_40_0.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Embedding, LSTM, Concatenate, Dense\n",
    "\n",
    "title_input = Input((None,), name=\"title\")\n",
    "title_embed = Embedding(1000, 64, name=\"embedding\")\n",
    "title_lstm = LSTM(128, name=\"lstm\", activation=\"sigmoid\")\n",
    "\n",
    "tags = Input((12,), name=\"tags\")\n",
    "concatenate = Concatenate(name=\"concatenate\",axis=-1)\n",
    "\n",
    "priority = Dense(1, name=\"priority\", activation=\"...\")\n",
    "\n",
    "title_embed_out = title_embed(title_input)\n",
    "title_lstm_out = title_lstm(title_embed_out)\n",
    "\n",
    "concatenate_all = concatenate([title_lstm_out, ..., ...])\n",
    "\n",
    "priority_out = priority(concatenate_all)\n",
    "\n",
    "model = keras.Model(inputs=[..., ..., ...], outputs=[..., ...])\n",
    "plot_model(model, \"figures/apinet_model.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training the model: fit() function (https://keras.io/api/models/model_training_apis/)\n",
    "\n",
    "* We are almost ready to train the model, I swear\n",
    "* fit() is a method of the Model, actually launches training on a dataset with features and labels\n",
    "* X_train, y_train: features and labels\n",
    "* batch: have we talked about stochastic gradient descent yet?\n",
    "* epochs: how many times we iterate through the dataset\n",
    "* validation_data: used to evaluate the model at the end of every epoch, NOT used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, batch=32, epochs=10, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training the model: fit() function (https://keras.io/api/models/model_training_apis/)\n",
    "\n",
    "* Ok, we miss the actual data, then we can train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, batch=32, epochs=10, validation_data=(X_val, y_val), class_weight = None, steps_per_epoch=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is this validation thing? Do I really need it?\n",
    "\n",
    "* Yes, yes you do\n",
    "* Helps understanding if the model is learning anything useful\n",
    "* Take some of your labelled data, set it aside, call it validation set and don't train on it\n",
    "* Evaluate model on validation set at the end of each epoch, see if model works on unseen data\n",
    "* If it works well on training set but not on validation set, you're overfitting\n",
    "\n",
    "<img src=\"figures/overfitting_class.png\" width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is this validation thing? Do I really need it?\n",
    "\n",
    "* If it works well on training set but not on validation set, you're overfitting\n",
    "* Validation data is used to adapt hyperparameters, select best models\n",
    "* Validation data is NOT testing data (those metrics do not go on papers!)\n",
    "* More on this next Thursday\n",
    "\n",
    "<img src=\"figures/early_stopping.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ok, can we PLEASE train a NN now?\n",
    "\n",
    "* Let's generate some artificial data, see what happens\n",
    "* Classification dataset, 2 classes\n",
    "* Let's say 10,000 samples, three features per sample\n",
    "* Random data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate dummy data\n",
    "data = np.random.random((10000, 3))\n",
    "labels = np.random.randint(2, size=(10000, 1))\n",
    "\n",
    "#let's print the first sample (three floats) and its corresponding label:\n",
    "print(np.hstack((data[0:10,:], labels[0:10])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## We have the data, now make the model, compile it, train it\n",
    "\n",
    "* Add Dropout for fun\n",
    "* Don't forget that softmax activation!\n",
    "* Batch size is 32, 10 epochs\n",
    "* Take 10% of the data, reserve it for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(4, input_dim=3, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(3, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "history = model.fit(data, labels, epochs=10, batch_size=32, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's visualize our training curves\n",
    "\n",
    "* Plots loss and accuracy for train and validation sets separately\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_loss_acc(history):\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train acc', 'val acc', 'train loss', 'val loss'], loc='upper left')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's visualize our training curves\n",
    "\n",
    "* Plots loss and accuracy for train and validation sets separately\n",
    "* The model didn't learn anything, which makes sense (data is random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_loss_acc(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Do it again, but with data that actually means something\n",
    "\n",
    "* Remember the XOR problem?\n",
    "* A perceptron is not able to separate XOR classes\n",
    "* A MLP should be able to\n",
    "\n",
    "<img src=\"figures/3-IP-TRUTH-TABLE2.jpg\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's generate data that behaves a bit like XOR:\n",
    "\n",
    "* positive numbers behave like a 1\n",
    "* negative numbers behave like a 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate XOR data\n",
    "data = np.random.random((10000, 3)) - 0.5\n",
    "labels = np.zeros((10000, 1))\n",
    "\n",
    "labels[np.where(np.logical_xor(np.logical_xor(data[:,0] > 0, data[:,1] > 0), data[:,2] > 0))] = 1\n",
    "\n",
    "#let's print some data and the corresponding label to check that they match the table above\n",
    "for x in range(5):\n",
    "    print(\"{0:.2f} xor {1:.2f} xor {2:.2f} equals {3:}\".format(data[x,0], data[x,1], data[x,2], labels[x,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(4, input_dim=3, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(3, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "history = model.fit(data, labels, epochs=10, batch_size=32, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## XOR data\n",
    "\n",
    "* Better than random!\n",
    "* Validation curves might be better than training curves because of Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_loss_acc(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise: can you do better?\n",
    "\n",
    "* More layers? More neurons?\n",
    "* Different activations?\n",
    "* Different Dropout? No Dropout?\n",
    "* Generate more data?\n",
    "* More epochs?\n",
    "* Different batch size?\n",
    "* Different optimizer?\n",
    "* It's up to you! Let's see who does best on validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(4, input_dim=3, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(3, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "history = model.fit(data, labels, epochs=10, batch_size=32, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regression problem: Boston housing dataset\n",
    "\n",
    "* Can we predict the value of a house (in tens of thousands of dollars) based on 13 variables:\n",
    "    *  CRIM     per capita crime rate by town\n",
    "    * ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "    * INDUS    proportion of non-retail business acres per town\n",
    "    * CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "    * NOX      nitric oxides concentration (parts per 10 million)\n",
    "    * RM       average number of rooms per dwelling\n",
    "    \n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Boston housing dataset (http://lib.stat.cmu.edu/datasets/boston)\n",
    "\n",
    "* Can you build a network to solve the problem?\n",
    "* What types of layers?\n",
    "* What types of activations?\n",
    "* Hint: NNs like inputs between -1 and +1, data needs scaling\n",
    "* Scale the output too? What if we didn't want to scale it?\n",
    "* Who can do this best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler #hint\n",
    "\n",
    "#This is how we load the dataset, pre-split in training/validation sets\n",
    "(X_train, y_train), (X_val, y_val) = tf.keras.datasets.boston_housing.load_data(path=\"boston_housing.npz\", test_split=0.2, seed=113)\n",
    "\n",
    "#Let's have a look at the data\n",
    "print(X_train.shape)\n",
    "print(X_train[0], y_train[0])\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(...)\n",
    "\n",
    "model.compile(...)\n",
    "history = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_loss_mae(history, metric):\n",
    "    \n",
    "    fig,ax = plt.subplots()\n",
    "    ax.plot(history.history[metric])\n",
    "    ax.plot(history.history['val_' + metric])\n",
    "    ax.set_ylabel(metric)\n",
    "    ax2=ax.twinx()\n",
    "    ax2.plot(history.history['loss'], c=\"red\")\n",
    "    ax2.plot(history.history['val_loss'], c=\"green\")\n",
    "    ax2.set_ylabel(\"loss\")\n",
    "    plt.title('model accuracy')\n",
    "    #plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train ' + metric, 'val ' + metric, 'train loss', 'val loss'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "plot_loss_mae(history, '...')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
