{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Intro to Keras, Tensorflow and advanced NN\n",
    "\n",
    "\n",
    "<center><img src=\"figures/keras-tensorflow-logo.jpg\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "* TensorFlow is an end-to-end open source platform for ML \n",
    "* It has a comprehensive, flexible ecosystem of tools and libraries \n",
    "* Allows to easily build and deploy ML powered applications.\n",
    "* Not only Neural Networks\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## https://keras.io\n",
    "\n",
    "* Keras is a high-level neural networks API (front-end), written in Python\n",
    "* Capable of running on top of TensorFlow, CNTK, or Theano (backends)\n",
    "* Built to simplify access to more complex backend libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## https://keras.io\n",
    "\n",
    "Use Keras if you need a deep learning library that:\n",
    "\n",
    "* Allows for easy and fast prototyping (through user friendliness, modularity, and extensibility).\n",
    "* Supports both convolutional networks and recurrent networks, as well as combinations of the two.\n",
    "* Runs seamlessly on CPU and GPU.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## https://tensorflow.org\n",
    "\n",
    "Use *TensorFlow* if you want a finer level of control:\n",
    "\n",
    "* Build your own NN layers\n",
    "* Personalized cost function\n",
    "* More complex architectures than those available on Keras\n",
    "    \n",
    "We will be mostly writing python code using Keras libraries, but \"under the hood\" Keras is using tensorflow libraries.\n",
    "\n",
    "The documentation is at [keras.io](https://keras.io)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## If you want to use R instead of python\n",
    "\n",
    "* Keras can run on R as well, it will look quite similar to the code in this notebook\n",
    "* (but, you know, with <- instead of =)\n",
    "* You find the docs here: https://keras.rstudio.com/\n",
    "* Probably better support if you stick with python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is a tensor\n",
    "\n",
    "The main variables in TensorFlow are, of course, tensors:\n",
    "\n",
    "> A tensor is often thought of as a generalized matrix. That is, it could be a 1-D matrix (a vector), a 3-D matrix (something like a cube of numbers), even a 0-D matrix (a single number), or a higher dimensional structure that is harder to visualize. The dimension of the tensor is called its rank.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is a tensor\n",
    "\n",
    "The main variables in TensorFlow are, of course, tensors:\n",
    "\n",
    "> A tensor is often thought of as a generalized matrix. That is, it could be a 1-D matrix (a vector), a 3-D matrix (something like a cube of numbers), even a 0-D matrix (a single number), or a higher dimensional structure that is harder to visualize. The dimension of the tensor is called its rank.\n",
    "\n",
    "## TensorFlow operates on tensors\n",
    "\n",
    "> TensorFlow computations are expressed as stateful dataflow graphs. The name TensorFlow derives from the operations that such neural networks perform on multidimensional data arrays, which are referred to as tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The first step is to build a graph of operations\n",
    "\n",
    "* NNs are defined in TensorFlow as graphs through which the data flows until the final result is produced\n",
    "* Before we can do any operation on our data (images, etc) we need to build the graph of tensor operations\n",
    "* When we have a full graph built from input to output, we can run (flow) our data (training or testing) through it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tensors and data are *not* the same thing\n",
    "* Tensors are, rather, a symbolic representation of the data\n",
    "* Think about the function $g = f(x)$: as long as we do not assign a value to $x$, we will not have a fully computed $g$\n",
    "* In this case, $g$ is the output tensor, $x$ the input tensor, $f$ the tensor operation (a Neural Network?)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example\n",
    "\n",
    "* We have a set of color images of size $1000x1000$ pixels (1 megapixel) that we want to use on our NN \n",
    "* We define tensors with shape $(n, 1000, 1000, 3)$\n",
    "    * $n$ is the number of images that we are presenting to our network in one go (a \"batch\")\n",
    "    * $1000x1000$: image pixels\n",
    "    * $3$ is the number of channels (RGB)\n",
    "    * Grayscale images tensors would have shape $(n, 1000, 1000, 1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## One thing to remember when operating on tensors\n",
    "\n",
    "The dimensions between tensors coming out of the $i$-th node and those going into the $(i+1)$-th node *must* match:\n",
    "\n",
    "* If each sample in our dataset is made of 10 features, the first (input) layer must accept a tensor of shape $(n, 10)$\n",
    "* If the first layer in our NN outputs a 3D tensor, the second layer must accept a 3D tensor as input\n",
    "* Check the documentation to make sure what input-output shapes are allowed ([example](https://keras.io/api/layers/convolution_layers/convolution1d/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Here's how a NN layer looks like in TensorFlow:\n",
    "\n",
    "* 7 samples in batch\n",
    "* 784 inputs\n",
    "* 500 outputs\n",
    "\n",
    "<center><img src=\"figures/run_metadata_graph.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Here is how a model is built and trained in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "```python\n",
    "#Multi-layer perceptron (one hidden layer)\n",
    "model = Sequential()\n",
    "model.add(Dense(3, input_dim=3, activation='sigmoid'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "#Gradient descent algorithm, Mean Squared Error as Loss function\n",
    "model.compile(optimizer='sgd', loss='mse', metrics=['mse'])\n",
    "\n",
    "#Training for 10 iterations of the data (epochs)\n",
    "history = model.fit(data, labels, epochs=10, batch=32)\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does each bit do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A neural network in Keras is called a Model\n",
    "\n",
    "The simplest kind of model is of the Sequential kind:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This is an \"empty\" model, with no layers, no inputs or outputs are defined either.\n",
    "\n",
    "Adding layer is easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "model = Sequential()\n",
    "model.add(Dense(units=3, activation='relu', input_dim=3))\n",
    "model.add(Dense(units=2, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "\n",
    "A \"Dense\" layer is a fully connected layer as the ones we have seen in Multi-layer Perceptrons.\n",
    "The above is equal to having this network:\n",
    "\n",
    "<center><img src=\"figures/simplenet.png\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If we want to see the layers in the Model this far, we can just call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Notice the number of parameters, can you tell why 12 and 8 parameters for each layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Using \"model.add()\" keeps stacking layers on top of what we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(units=2, activation=None))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "One can also declare the model in one go, by passing a list of layers to Sequential() like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(units=3, activation='relu', input_dim=3),\n",
    "    Dense(units=2, activation='softmax'),\n",
    "    Dense(units=2, activation=None)\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If we want to see the layers in the Model this far, we can just call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "plot_model(model, \"figures/simplenet_model.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small exercise\n",
    "\n",
    "* Can you write code to make a simple NN model on Keras?\n",
    "* Open the `exercises` jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Keras layers (https://keras.io/api/layers/)\n",
    "\n",
    "Common layers (we will cover most of these!)\n",
    "\n",
    "* Trainable\n",
    "    * <font color='red'>Dense (fully connected/MLP)</font>\n",
    "    * Conv1D (2D/3D)\n",
    "    * Recurrent: LSTM/GRU/Bidirectional\n",
    "\n",
    "\n",
    "* Non-trainable\n",
    "    * <font color='red'>Dropout</font>\n",
    "    * Flatten\n",
    "    * MaxPooling1D (2D/3D)\n",
    "    * Merge (add/subtract/concatenate)\n",
    "    * <font color='red'>Activation (Softmax/ReLU/Sigmoid/...)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Compiling a model\n",
    "\n",
    "Once we have defined a model we want to \"compile\" it\n",
    "\n",
    "This means chosing a Loss function and an Optimizer (the algorithm that finds the minimum loss possible)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',                    #adaptive learning rate method\n",
    "              loss='sparse_categorical_crossentropy', #loss function for classification problems with integer labels\n",
    "              metrics=['accuracy'])                   #the metric doesn't influence the training\n",
    "\n",
    "model.optimizer.get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Losses (https://keras.io/api/losses/)\n",
    "\n",
    "These are the functions used to evaluate and train the neural network\n",
    "\n",
    "Common losses for classification problems:\n",
    "* CategoricalCrossentropy\n",
    "* SparseCategoricalCrossentropy\n",
    "* KLDivergence\n",
    "\n",
    "Common losses for regression problems:\n",
    "* MeanSquaredError\n",
    "* MeanAbsoluteError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Metrics (https://keras.io/api/metrics/)\n",
    "\n",
    "Common metrics for classification:\n",
    "* Accuracy/CategoricalAccuracy (respectively for integer labels or one-hot labels)\n",
    "* SparseCategoricalCrossentropy/CategoricalCrossentropy (integer/one-hot labels)\n",
    "* Precision/Recall\n",
    "* AUC\n",
    "\n",
    "Common metrics for regression:\n",
    "* MeanSquaredError\n",
    "* MeanAbsoluteError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Metrics (https://keras.io/api/metrics/)\n",
    "\n",
    "Notice the \"metrics\" parameter, which accepts a list of values. Multiple metrics can be shown during training.\n",
    "Metrics are only to visualize how the training is going, they don't have an effect on training itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "model.compile(optimizer=RMSprop(learning_rate=1.0),   #adaptive learning rate method\n",
    "              loss='sparse_categorical_crossentropy', #loss function for classification problems with integer labels\n",
    "              metrics=['accuracy', 'recall'])         #the metric doesn't influence the training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimizers (https://keras.io/api/optimizers/)\n",
    "\n",
    "* They are algorithms for gradient descent\n",
    "* A few to choose from:\n",
    "    * SGD (Stochastic Gradient Descent)\n",
    "    * RMSprop (Root Mean Square propagation)\n",
    "    * Adadelta (Adaptive delta)\n",
    "    * Adam (Adaptive Moment estimation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<img src=\"figures/gradient_descent.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient Descent \n",
    "\n",
    "We have seen how gradient descent works:\n",
    "\n",
    "For each epoch:\n",
    "* Get predicted $y$ ($Å·$) for all $N$ samples\n",
    "* Calculate error (loss)\n",
    "* Calculate all gradients (backprop)\n",
    "* Apply gradients to weights\n",
    "    \n",
    "Pros/cons:\n",
    "* Stable procedure\n",
    "* Guarantees lower error at next step\n",
    "* Will get stuck at local minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<img src=\"figures/gradient_descent.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stochastic Gradient Descent\n",
    "For each epoch:\n",
    "* Divide data in batch blocks of size $n < N$\n",
    "* For each of the $N/n$ blocks:\n",
    "    * Get predicted $y$ for $n$ samples\n",
    "    * Calculate partial loss\n",
    "    * Calculate gradients (backprop)\n",
    "    * Apply gradients to weights\n",
    "\n",
    "Pros/cons:\n",
    "* Noisy gradients\n",
    "* Error will still go down overall\n",
    "* Less likely to get stuck at local minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<img src=\"figures/gradient_descent.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimizers (https://keras.io/api/optimizers/)\n",
    "\n",
    "We need to choose a learning rate to multiply to our gradient. If it is too small, we risk taking too long to get to a minimum\n",
    "<center><img src=\"figures/small_lr.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideOutput": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimizers (https://keras.io/api/optimizers/)\n",
    "\n",
    "If it is too large, the network risks becoming unstable, explode\n",
    "\n",
    "<center><img src=\"figures/large_lr.png\"></center>\n",
    "\n",
    "Let's test different optimization strategies on Tensorflow playground: http://playground.tensorflow.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimizers (https://keras.io/api/optimizers/)\n",
    "\n",
    "Luckily there are algorithms to address these issues:\n",
    "* Increase descent speed when past gradients agree with current, slow down otherwise (momentum)\n",
    "* Annealing (decrease learning rate with passing time)\n",
    "* Different learning rates for different parameters\n",
    "* Adaptive learning rate based on gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<img src=\"figures/adaptive_lr.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimizers (https://keras.io/api/optimizers/)\n",
    "\n",
    "* They are algorithms for gradient descent\n",
    "* A few to choose from:\n",
    "    * SGD (stochastic gradient descent)\n",
    "        * One learning rate, fixed\n",
    "        * Old, but works well with Nesterov momentum\n",
    "    * RMSprop\n",
    "        * One learning rate per parameter\n",
    "        * Adaptive learning rate (divide by squared mean of past gradients)\n",
    "    * Adadelta (adaptive learning rate)\n",
    "        * Similar to RMSprop, no need to set initial learning rate\n",
    "    * Adam (Adaptive moment estimation)\n",
    "        * Combines pros from RMSprop, Adadelta, works well with most problems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimizers (https://keras.io/api/optimizers/)\n",
    "<br>\n",
    "<br>\n",
    "<center><img src=\"figures/adam_et_al.png\" width=500></center>\n",
    "<div style=\"text-align: right\">(\"Adam: A Method for Stochastic Optimization\", 2015)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training the model: fit() function (https://keras.io/api/models/model_training_apis/)\n",
    "\n",
    "* We are almost ready to train the model, I swear\n",
    "* fit() is a method of the Model, actually launches training on a dataset with features and labels\n",
    "* X_train, y_train: features and labels\n",
    "* batch: how many samples between each weight update\n",
    "* epochs: how many times we iterate through the dataset\n",
    "* validation_data: used to evaluate the model at the end of every epoch, NOT used for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), batch=32, epochs=10)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training the model: fit() function (https://keras.io/api/models/model_training_apis/)\n",
    "\n",
    "* Ok, last thing we need is the actual data, then we can train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), batch=32, epochs=10, validation_data=(X_val, y_val))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is this validation thing? Do I really need it?\n",
    "\n",
    "* Yes, yes you do\n",
    "* Helps understanding if the model is learning anything useful\n",
    "* Take some of your labelled data, set it aside, call it validation set and don't train on it\n",
    "* Evaluate model on validation set at the end of each epoch, see if model works on unseen data\n",
    "* If it works well on training set but not on validation set, you're overfitting\n",
    "\n",
    "<img src=\"figures/overfitting_class.png\" width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is this validation thing? Do I really need it?\n",
    "\n",
    "* If it works well on training set but not on validation set, you're overfitting\n",
    "* Validation data is used to adapt hyperparameters, select best models\n",
    "* Validation data is NOT testing data (more on this later)\n",
    "* Let's try this on Tensorflow playground: http://playground.tensorflow.org\n",
    "\n",
    "<img src=\"figures/early_stopping.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ok, can we PLEASE train a NN now?\n",
    "\n",
    "* Let's generate some artificial data, see what happens\n",
    "* Classification dataset, 2 classes\n",
    "* Let's say 10,000 samples, three features per sample\n",
    "* Random data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate dummy data\n",
    "data = np.random.random((10000, 3))\n",
    "labels = np.random.randint(2, size=(10000, 1))\n",
    "\n",
    "#let's print the first sample (three floats) and its corresponding label:\n",
    "print(np.hstack((data[0:10,:], labels[0:10])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## We have the data, now make the model, compile it, train it\n",
    "\n",
    "* At the last layer of a classifier use the _softmax_ activation (more on this later)\n",
    "* Batch size is 32, 10 epochs\n",
    "* Take 10% of the data, reserve it for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(4, input_dim=3, activation='sigmoid'))\n",
    "model.add(Dense(3, activation='sigmoid'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "history = model.fit(data, labels, epochs=10, batch_size=32, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's visualize our training curves\n",
    "\n",
    "* Plots loss and accuracy for train and validation sets separately\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(4, input_dim=3, activation='tanh'))\n",
    "model.add(Dense(3, activation='tanh'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "history = model.fit(data, labels, epochs=100, batch_size=32, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_loss_acc(history):\n",
    "    try:\n",
    "        plt.plot(history.history['accuracy'])\n",
    "        plt.plot(history.history['val_accuracy'])\n",
    "    except:\n",
    "        plt.plot(history.history['acc'])\n",
    "        plt.plot(history.history['val_acc'])\n",
    "\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train acc', 'val acc', 'train loss', 'val loss'], loc='upper left')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's visualize our training curves\n",
    "\n",
    "* Plots loss and accuracy for train and validation sets separately\n",
    "* The model didn't learn anything, which makes sense (data is random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_loss_acc(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Do it again, but with data that actually means something\n",
    "\n",
    "* A XOR function is not linear\n",
    "* A perceptron is not able to separate XOR classes\n",
    "* A MLP should be able to\n",
    "\n",
    "<img src=\"figures/3-IP-TRUTH-TABLE2.jpg\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's generate data that is not just binary, but behaves like it:\n",
    "\n",
    "* A positive (+) input behaves like a 1\n",
    "* A negative (-) input behaves like a 0\n",
    "* -0.5 $\\oplus$ 0.2 $\\oplus$ -0.1 => 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Generate XOR data\n",
    "data = np.random.random((10000, 3)) - 0.5\n",
    "labels = np.zeros((10000, 1))\n",
    "\n",
    "labels[np.where(np.logical_xor(np.logical_xor(data[:,0] > 0, data[:,1] > 0), data[:,2] > 0))] = 1\n",
    "\n",
    "#let's print some data and the corresponding label to check that they match the table above\n",
    "for x in range(3):\n",
    "    print(\"{0: .2f} xor {1: .2f} xor {2: .2f} equals {3:}\".format(data[x,0], data[x,1], data[x,2], labels[x,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "transformed = pca.fit_transform(data)\n",
    "plt.scatter(transformed[:,0], transformed[:,1], c=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now let's fit a model to the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(4, input_dim=3, activation='tanh'))\n",
    "model.add(Dense(3, activation='tanh'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "history = model.fit(data, labels, epochs=10, batch_size=32, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## XOR data\n",
    "\n",
    "* Better than random!\n",
    "* Notice the difference between train and validation curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_loss_acc(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise: can you do better?\n",
    "\n",
    "* Check the exercise notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(4, input_dim=3, activation='sigmoid'))\n",
    "model.add(Dense(3, activation='sigmoid'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "history = model.fit(data, labels, epochs=10, batch_size=32, validation_split=0.1)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "rise": {
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
